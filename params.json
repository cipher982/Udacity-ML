{
  "name": "Udacity-ml",
  "tagline": "Collections of coursework and personal projects from the Udacity ML Program",
  "body": "\r\n# Machine Learning Engineer Nanodegree\r\n## Model Evaluation & Validation\r\n## Project 1: Predicting Boston Housing Prices\r\n\r\nWelcome to the first project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been written. You will need to implement additional functionality to successfully answer all of the questions for this project. Unless it is requested, do not modify any of the code that has already been included. In this template code, there are four sections which you must complete to successfully produce a prediction with your model. Each section where you will write code is preceded by a **STEP X** header with comments describing what must be done. Please read the instructions carefully!\r\n\r\nIn addition to implementing code, there will be questions that you must answer that relate to the project and your implementation. Each section where you will answer a question is preceded by a **QUESTION X** header. Be sure that you have carefully read each question and provide thorough answers in the text boxes that begin with \"**Answer:**\". Your project submission will be evaluated based on your answers to each of the questions.  \r\n\r\nA description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Housing), which is provided by the **UCI Machine Learning Repository**.\r\n\r\n# Getting Started\r\nTo familiarize yourself with an iPython Notebook, **try double clicking on this cell**. You will notice that the text changes so that all the formatting is removed. This allows you to make edits to the block of text you see here. This block of text (and mostly anything that's not code) is written using [Markdown](http://daringfireball.net/projects/markdown/syntax), which is a way to format text using headers, links, italics, and many other options! Whether you're editing a Markdown text block or a code block (like the one below), you can use the keyboard shortcut **Shift + Enter** or **Shift + Return** to execute the code or text block. In this case, it will show the formatted text.\r\n\r\nLet's start by setting up some code we will need to get the rest of the project up and running. Use the keyboard shortcut mentioned above on the following code block to execute it. Alternatively, depending on your iPython Notebook program, you can press the **Play** button in the hotbar. You'll know the code block executes successfully if the message *\"Boston Housing dataset loaded successfully!\"* is printed.\r\n\r\n\r\n```python\r\n# Importing a few necessary libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as pl\r\nfrom sklearn import datasets\r\nfrom sklearn.tree import DecisionTreeRegressor\r\n\r\n# Make matplotlib show our plots inline (nicely formatted in the notebook)\r\n%matplotlib inline\r\n\r\n# Create our client's feature set for which we will be predicting a selling price\r\nCLIENT_FEATURES = [[11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]]\r\n\r\n\r\n# Load the Boston Housing dataset into the city_data variable\r\ncity_data = datasets.load_boston()\r\n\r\n# Initialize the housing prices and housing features\r\nhousing_prices = city_data.target\r\nhousing_features = city_data.data\r\n\r\n\r\nmeanss = np.mean(housing_features, axis = 0)\r\nprint meanss.astype(int)\r\n\r\n    \r\nprint \"Boston Housing dataset loaded successfully!\"\r\n```\r\n\r\n    [  3  11  11   0   0   6  68   3   9 408  18 356  12]\r\n    Boston Housing dataset loaded successfully!\r\n    \r\n\r\n# Statistical Analysis and Data Exploration\r\nIn this first section of the project, you will quickly investigate a few basic statistics about the dataset you are working with. In addition, you'll look at the client's feature set in `CLIENT_FEATURES` and see how this particular sample relates to the features of the dataset. Familiarizing yourself with the data through an explorative process is a fundamental practice to help you better understand your results.\r\n\r\n## Step 1\r\nIn the code block below, use the imported `numpy` library to calculate the requested statistics. You will need to replace each `None` you find with the appropriate `numpy` coding for the proper statistic to be printed. Be sure to execute the code block each time to test if your implementation is working successfully. The print statements will show the statistics you calculate!\r\n\r\n\r\n```python\r\n# Number of houses in the dataset\r\ntotal_houses   = np.count_nonzero(housing_prices)\r\n\r\n# Number of features in the dataset\r\ntotal_features = city_data.data.shape[1]\r\n\r\n# Minimum housing value in the dataset\r\nminimum_price  = np.amin(city_data.target)\r\n\r\n# Maximum housing value in the dataset\r\nmaximum_price  = np.amax(city_data.target)\r\n\r\n# Mean house value of the dataset\r\nmean_price     = np.average(city_data.target)\r\n\r\n# Median house value of the dataset\r\nmedian_price   = np.median(city_data.target)\r\n\r\n# Standard deviation of housing values of the dataset\r\nstd_dev        = np.std(city_data.target)\r\n\r\n# Show the calculated statistics\r\nprint \"Boston Housing dataset statistics (in $1000's):\\n\"\r\nprint \"Total number of houses:\", total_houses\r\nprint \"Total number of features:\", total_features\r\nprint \"Minimum house price:\", minimum_price\r\nprint \"Maximum house price:\", maximum_price\r\nprint \"Mean house price: {0:.3f}\".format(mean_price)\r\nprint \"Median house price:\", median_price\r\nprint \"Standard deviation of house price: {0:.3f}\".format(std_dev)\r\n```\r\n\r\n    Boston Housing dataset statistics (in $1000's):\r\n    \r\n    Total number of houses: 506\r\n    Total number of features: 13\r\n    Minimum house price: 5.0\r\n    Maximum house price: 50.0\r\n    Mean house price: 22.533\r\n    Median house price: 21.2\r\n    Standard deviation of house price: 9.188\r\n    \r\n\r\n\r\n```python\r\nprint CLIENT_FEATURES\r\n```\r\n\r\n    [[11.95, 0.0, 18.1, 0, 0.659, 5.609, 90.0, 1.385, 24, 680.0, 20.2, 332.09, 12.13]]\r\n    \r\n\r\n# Evaluating Model Performance\r\nIn this second section of the project, you will begin to develop the tools necessary for a model to make a prediction. Being able to accurately evaluate each model's performance through the use of these tools helps to greatly reinforce the confidence in your predictions.\r\n\r\n## Step 2\r\nIn the code block below, you will need to implement code so that the `shuffle_split_data` function does the following:\r\n- Randomly shuffle the input data `X` and target labels (housing values) `y`.\r\n- Split the data into training and testing subsets, holding 30% of the data for testing.\r\n\r\nIf you use any functions not already acessible from the imported libraries above, remember to include your import statement below as well!   \r\nEnsure that you have executed the code block once you are done. You'll know the `shuffle_split_data` function is working if the statement *\"Successfully shuffled and split the data!\"* is printed.\r\n\r\n\r\n```python\r\n# Put any import statements you need for this code block here\r\nimport sklearn.metrics as sk\r\n\r\n\r\ndef shuffle_split_data(X, y):\r\n    \"\"\" Shuffles and splits data into 70% training and 30% testing subsets,\r\n        then returns the training and testing subsets. \"\"\"\r\n\r\n    # Shuffle and split the data\r\n\r\n    X_train, X_test, y_train, y_test = cv.train_test_split(X, y, test_size=0.30, random_state=1)\r\n\r\n    # Return the training and testing data subsets\r\n    return X_train, y_train, X_test, y_test\r\n\r\n\r\n# Test shuffle_split_data\r\ntry:\r\n    X_train, y_train, X_test, y_test = shuffle_split_data(housing_features, housing_prices)\r\n    print \"Successfully shuffled and split the data!\"\r\nexcept:\r\n    print \"Something went wrong with shuffling and splitting the data.\"\r\n```\r\n\r\n    Successfully shuffled and split the data!\r\n    \r\n\r\n## Step 3\r\nIn the code block below, you will need to implement code so that the `performance_metric` function does the following:\r\n- Perform a total error calculation between the true values of the `y` labels `y_true` and the predicted values of the `y` labels `y_predict`.\r\n\r\nYou will need to first choose an appropriate performance metric for this problem. See [the sklearn metrics documentation](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) to view a list of available metric functions. **Hint: ** Look at the question below to see a list of the metrics that were covered in the supporting course for this project.\r\n\r\nOnce you have determined which metric you will use, remember to include the necessary import statement as well!  \r\nEnsure that you have executed the code block once you are done. You'll know the `performance_metric` function is working if the statement *\"Successfully performed a metric calculation!\"* is printed.\r\n\r\n\r\n```python\r\n# Put any import statements you need for this code block here\r\nimport sklearn.metrics as sk\r\n\r\n\r\ndef performance_metric(y_true, y_predict):\r\n    \"\"\" Calculates and returns the total error between true and predicted values\r\n        based on a performance metric chosen by the student. \"\"\"\r\n\r\n    error = sk.mean_squared_error(y_true, y_predict)\r\n    print error\r\n    return error\r\n\r\n\r\n# Test performance_metric\r\ntry:\r\n    total_error = performance_metric(y_train, y_train)\r\n    print \"Successfully performed a metric calculation!\"\r\nexcept:\r\n    print \"Something went wrong with performing a metric calculation.\"\r\n```\r\n\r\n    0.0\r\n    Successfully performed a metric calculation!\r\n    \r\n\r\n## Step 4 (Final Step)\r\nIn the code block below, you will need to implement code so that the `fit_model` function does the following:\r\n- Create a scoring function using the same performance metric as in **Step 2**. See the [sklearn `make_scorer` documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\r\n- Build a GridSearchCV object using `regressor`, `parameters`, and `scoring_function`. See the [sklearn documentation on GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html).\r\n\r\nWhen building the scoring function and GridSearchCV object, *be sure that you read the parameters documentation thoroughly.* It is not always the case that a default parameter for a function is the appropriate setting for the problem you are working on.\r\n\r\nSince you are using `sklearn` functions, remember to include the necessary import statements below as well!  \r\nEnsure that you have executed the code block once you are done. You'll know the `fit_model` function is working if the statement *\"Successfully fit a model to the data!\"* is printed.\r\n\r\n\r\n```python\r\n# Put any import statements you need for this code block\r\nfrom sklearn import grid_search as gs\r\nfrom sklearn import metrics as m\r\n\r\n\r\ndef fit_model(X, y):\r\n    \"\"\" Tunes a decision tree regressor model using GridSearchCV on the input data X \r\n        and target labels y and returns this optimal model. \"\"\"\r\n\r\n    # Create a decision tree regressor object\r\n    regressor = DecisionTreeRegressor()\r\n\r\n    # Set up the parameters we wish to tune\r\n    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}\r\n\r\n    # Make an appropriate scoring function\r\n    scoring_function = m.make_scorer(m.mean_squared_error, greater_is_better = False)\r\n\r\n    # Make the GridSearchCV object\r\n    reg = gs.GridSearchCV(regressor,parameters, scoring=scoring_function)\r\n\r\n    # Fit the learner to the data to obtain the optimal model with tuned parameters\r\n    reg.fit(X, y)\r\n\r\n    # Return the optimal model\r\n    return reg.best_estimator_\r\n\r\n\r\n# Test fit_model on entire dataset\r\ntry:\r\n    reg = fit_model(housing_features, housing_prices)\r\n    print \"Successfully fit a model!\"\r\nexcept:\r\n    print \"Something went wrong with fitting a model.\"\r\n```\r\n\r\n    Successfully fit a model!\r\n    \r\n\r\n# Checkpoint!\r\nYou have now successfully completed your last code implementation section. Pat yourself on the back! All of your functions written above will be executed in the remaining sections below, and questions will be asked about various results for you to analyze. To prepare the **Analysis** and **Prediction** sections, you will need to intialize the two functions below. Remember, there's no need to implement any more code, so sit back and execute the code blocks! Some code comments are provided if you find yourself interested in the functionality.\r\n\r\n\r\n```python\r\ndef learning_curves(X_train, y_train, X_test, y_test):\r\n    \"\"\" Calculates the performance of several models with varying sizes of training data.\r\n        The learning and testing error rates for each model are then plotted. \"\"\"\r\n    \r\n    print \"Creating learning curve graphs for max_depths of 1, 3, 6, and 10. . .\"\r\n    \r\n    # Create the figure window\r\n    fig = pl.figure(figsize=(10,8))\r\n\r\n    # We will vary the training set size so that we have 50 different sizes\r\n    sizes = np.rint(np.linspace(1, len(X_train), 50)).astype(int)\r\n    train_err = np.zeros(len(sizes))\r\n    test_err = np.zeros(len(sizes))\r\n\r\n    # Create four different models based on max_depth\r\n    for k, depth in enumerate([1,3,6,10]):\r\n        \r\n        for i, s in enumerate(sizes):\r\n            \r\n            # Setup a decision tree regressor so that it learns a tree with max_depth = depth\r\n            regressor = DecisionTreeRegressor(max_depth = depth)\r\n            \r\n            # Fit the learner to the training data\r\n            regressor.fit(X_train[:s], y_train[:s])\r\n\r\n            # Find the performance on the training set\r\n            train_err[i] = performance_metric(y_train[:s], regressor.predict(X_train[:s]))\r\n            \r\n            # Find the performance on the testing set\r\n            test_err[i] = performance_metric(y_test, regressor.predict(X_test))\r\n\r\n        # Subplot the learning curve graph\r\n        ax = fig.add_subplot(2, 2, k+1)\r\n        ax.plot(sizes, test_err, lw = 2, label = 'Testing Error')\r\n        ax.plot(sizes, train_err, lw = 2, label = 'Training Error')\r\n        ax.legend()\r\n        ax.set_title('max_depth = %s'%(depth))\r\n        ax.set_xlabel('Number of Data Points in Training Set')\r\n        ax.set_ylabel('Total Error')\r\n        ax.set_xlim([0, len(X_train)])\r\n    \r\n    # Visual aesthetics\r\n    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize=18, y=1.03)\r\n    fig.tight_layout()\r\n    fig.show()\r\n```\r\n\r\n\r\n```python\r\ndef model_complexity(X_train, y_train, X_test, y_test):\r\n    \"\"\" Calculates the performance of the model as model complexity increases.\r\n        The learning and testing errors rates are then plotted. \"\"\"\r\n    \r\n    print \"Creating a model complexity graph. . . \"\r\n\r\n    # We will vary the max_depth of a decision tree model from 1 to 14\r\n    max_depth = np.arange(1, 14)\r\n    train_err = np.zeros(len(max_depth))\r\n    test_err = np.zeros(len(max_depth))\r\n\r\n    for i, d in enumerate(max_depth):\r\n        # Setup a Decision Tree Regressor so that it learns a tree with depth d\r\n        regressor = DecisionTreeRegressor(max_depth = d)\r\n\r\n        # Fit the learner to the training data\r\n        regressor.fit(X_train, y_train)\r\n\r\n        # Find the performance on the training set\r\n        train_err[i] = performance_metric(y_train, regressor.predict(X_train))\r\n\r\n        # Find the performance on the testing set\r\n        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\r\n\r\n    # Plot the model complexity graph\r\n    pl.figure(figsize=(7, 5))\r\n    pl.title('Decision Tree Regressor Complexity Performance')\r\n    pl.plot(max_depth, test_err, lw=2, label = 'Testing Error')\r\n    pl.plot(max_depth, train_err, lw=2, label = 'Training Error')\r\n    pl.legend()\r\n    pl.xlabel('Maximum Depth')\r\n    pl.ylabel('Total Error')\r\n    pl.show()\r\n```\r\n\r\n# Analyzing Model Performance\r\nIn this third section of the project, you'll take a look at several models' learning and testing error rates on various subsets of training data. Additionally, you'll investigate one particular algorithm with an increasing `max_depth` parameter on the full training set to observe how model complexity affects learning and testing errors. Graphing your model's performance based on varying criteria can be beneficial in the analysis process, such as visualizing behavior that may not have been apparent from the results alone.\r\n\r\n\r\n```python\r\nlearning_curves(X_train, y_train, X_test, y_test)\r\n```\r\n\r\n    Creating learning curve graphs for max_depths of 1, 3, 6, and 10. . .\r\n    0.0\r\n    98.3224342105\r\n    8.03729166667\r\n    84.7365497076\r\n    21.8544761905\r\n    70.7667991407\r\n    23.1092339545\r\n    61.1647255042\r\n    22.6923214286\r\n    72.2093682029\r\n    28.4026718147\r\n    72.1955344791\r\n    28.2421861472\r\n    72.5744462346\r\n    44.6502208014\r\n    63.3395175803\r\n    40.5614915254\r\n    63.259452924\r\n    39.2708912072\r\n    57.7902117413\r\n    35.883516116\r\n    57.7762710498\r\n    38.3001368243\r\n    57.7035356075\r\n    37.9522341045\r\n    57.7507873792\r\n    38.463155418\r\n    57.719004389\r\n    40.2347602827\r\n    60.748646112\r\n    41.9236826629\r\n    60.7703621456\r\n    42.9111832252\r\n    60.6826006721\r\n    43.8054770249\r\n    60.7831630425\r\n    43.1595830153\r\n    60.8720775391\r\n    43.0599659974\r\n    53.1418584726\r\n    43.4140491641\r\n    53.1057795338\r\n    43.4483012342\r\n    53.1070069404\r\n    42.6289119167\r\n    53.1125756047\r\n    41.3856158326\r\n    53.1541356251\r\n    40.4582730389\r\n    53.1481182331\r\n    40.0343493947\r\n    53.1558947019\r\n    40.3570943276\r\n    53.235242432\r\n    39.5149471983\r\n    53.2333562489\r\n    39.5835361386\r\n    53.1771623394\r\n    41.3269090732\r\n    53.1803770546\r\n    40.9301296962\r\n    53.1292070883\r\n    40.0715566388\r\n    53.1591725519\r\n    39.5024007757\r\n    53.1584572394\r\n    39.8995693132\r\n    53.2390284667\r\n    41.4494614244\r\n    56.3215869365\r\n    42.4568140801\r\n    53.1363012498\r\n    42.6158107737\r\n    53.1407116045\r\n    43.603990571\r\n    61.3851039297\r\n    43.9062506887\r\n    61.5535811162\r\n    43.5387623694\r\n    61.6326675225\r\n    45.9954481628\r\n    53.2497946861\r\n    46.2489900056\r\n    54.298310604\r\n    46.1315919384\r\n    54.2687661071\r\n    45.6047181481\r\n    54.2967786594\r\n    45.4273819329\r\n    54.2445341495\r\n    45.0757477846\r\n    54.2064264255\r\n    44.7618472132\r\n    54.1817392761\r\n    44.1806192316\r\n    54.1813454764\r\n    43.7395408424\r\n    54.158010876\r\n    44.6808032477\r\n    54.1464733818\r\n    0.0\r\n    98.3224342105\r\n    0.0408333333333\r\n    68.2269444444\r\n    3.92893333333\r\n    53.3334151316\r\n    3.04708074534\r\n    36.9376597744\r\n    5.41435858586\r\n    51.0740121384\r\n    6.7191722973\r\n    47.583228053\r\n    6.51004786574\r\n    35.7275264831\r\n    13.6877615025\r\n    55.5049990213\r\n    13.7464381759\r\n    53.1191352684\r\n    13.9670885616\r\n    49.1969397666\r\n    13.0815586653\r\n    50.8334470821\r\n    11.8691444619\r\n    50.1435293555\r\n    12.0661845592\r\n    55.3630977196\r\n    12.1875604056\r\n    46.2803445276\r\n    11.6109034462\r\n    31.8636033596\r\n    12.5039310713\r\n    39.8654795001\r\n    12.7122574507\r\n    32.1163912225\r\n    12.8821424261\r\n    32.1125912577\r\n    12.8089975446\r\n    32.229188181\r\n    13.5322835184\r\n    35.2391783407\r\n    13.7745841406\r\n    20.3456602448\r\n    13.8868690798\r\n    30.6148372348\r\n    13.6064269329\r\n    35.4010598535\r\n    13.5646338121\r\n    24.18663113\r\n    13.894445682\r\n    25.9845459925\r\n    13.7359754059\r\n    31.5642190526\r\n    14.9550107394\r\n    25.2487469515\r\n    14.5396325714\r\n    30.3220325687\r\n    14.6305643324\r\n    17.5648429739\r\n    13.8556368775\r\n    30.5339075705\r\n    13.8225033561\r\n    30.1822861116\r\n    13.3452279804\r\n    30.4653851761\r\n    13.5883473774\r\n    38.5614451146\r\n    13.9898833943\r\n    31.1383744437\r\n    14.5982279202\r\n    21.7764731065\r\n    16.5294474737\r\n    34.0557909535\r\n    16.8083988203\r\n    33.490565682\r\n    15.4298847992\r\n    28.0459242963\r\n    15.3837370723\r\n    28.0346409386\r\n    15.7292320446\r\n    28.3684674445\r\n    17.1916893553\r\n    31.3718417546\r\n    16.4855634005\r\n    20.3306116827\r\n    16.3913024833\r\n    20.3691110774\r\n    16.3852216826\r\n    20.4160943402\r\n    16.3346453093\r\n    19.858015733\r\n    16.3113045147\r\n    29.9194260175\r\n    16.3626727765\r\n    18.465570053\r\n    16.4829684442\r\n    18.5342407562\r\n    16.3198363958\r\n    18.4278005684\r\n    16.2582331003\r\n    17.7389846107\r\n    0.0\r\n    98.3224342105\r\n    0.0\r\n    70.8125657895\r\n    0.195666666667\r\n    55.7856414474\r\n    0.0428260869565\r\n    40.3136513158\r\n    0.196166666667\r\n    58.7248848684\r\n    0.148423423423\r\n    53.6934548246\r\n    0.270691287879\r\n    39.3068799114\r\n    1.29988795518\r\n    36.0620150525\r\n    1.46389612224\r\n    39.549739162\r\n    2.27667846043\r\n    49.6865598821\r\n    2.06449543379\r\n    39.1518122076\r\n    1.92385885989\r\n    44.6703657398\r\n    1.66134619595\r\n    41.0911235164\r\n    1.41869014202\r\n    43.4873844461\r\n    1.57256325864\r\n    54.2097493317\r\n    1.65615820724\r\n    28.7462713667\r\n    1.8703640952\r\n    31.6028992533\r\n    2.00993407293\r\n    27.2226634162\r\n    2.272193135\r\n    29.2977779144\r\n    1.96465145503\r\n    16.0523597798\r\n    2.38016399463\r\n    31.1034867447\r\n    2.20798149256\r\n    24.4936258125\r\n    2.35001996682\r\n    24.8354799432\r\n    1.91358477281\r\n    18.7876283968\r\n    2.13292091873\r\n    16.2880367674\r\n    2.11802174268\r\n    18.4201935598\r\n    2.25318927196\r\n    18.4436391621\r\n    2.77362099352\r\n    17.2561625935\r\n    2.80872589473\r\n    21.0618284788\r\n    2.25119066952\r\n    42.5882574377\r\n    2.58237463059\r\n    35.115248053\r\n    2.3840490892\r\n    29.5490680159\r\n    2.50914761554\r\n    44.8752948743\r\n    2.83779570347\r\n    36.1138682343\r\n    3.43780105213\r\n    14.6075223764\r\n    2.89123768693\r\n    39.0402507196\r\n    3.3488545263\r\n    42.7279214979\r\n    4.4265031763\r\n    17.592542811\r\n    4.35180409384\r\n    16.5715008199\r\n    4.85812521382\r\n    17.6917176102\r\n    3.9023296193\r\n    17.0758817219\r\n    3.97308641554\r\n    16.9019198956\r\n    3.90460727621\r\n    15.2659839188\r\n    3.92033350785\r\n    14.2757839242\r\n    4.26613575608\r\n    14.4173673409\r\n    4.16392459984\r\n    13.7221119365\r\n    4.47275252777\r\n    14.9743426279\r\n    4.52712812522\r\n    14.6423274622\r\n    4.50496606448\r\n    17.5646611531\r\n    4.17594595812\r\n    17.7323013399\r\n    0.0\r\n    98.3224342105\r\n    0.0\r\n    71.93875\r\n    0.0\r\n    51.2001315789\r\n    0.0\r\n    43.5972368421\r\n    0.000166666666667\r\n    51.7109375\r\n    0.0\r\n    54.5478947368\r\n    0.0\r\n    34.5734868421\r\n    0.00254901960784\r\n    41.0729440789\r\n    0.00135593220339\r\n    45.0478947368\r\n    0.11253030303\r\n    51.6331625\r\n    0.00414383561644\r\n    40.6502919408\r\n    0.00640625\r\n    43.6283347039\r\n    0.000229885057471\r\n    54.2674342105\r\n    0.000105263157895\r\n    35.3494901316\r\n    0.00625816993464\r\n    53.6336494883\r\n    0.00100917431193\r\n    36.7789638158\r\n    0.00113505747126\r\n    31.3443932749\r\n    0.00172086720867\r\n    28.5729532164\r\n    0.00284987277354\r\n    36.367496345\r\n    0.089568236715\r\n    21.3009730217\r\n    0.0479574712644\r\n    21.0888025219\r\n    0.0524013157895\r\n    17.7408589181\r\n    0.0634538784067\r\n    21.9837385782\r\n    0.0291846307385\r\n    21.4777271382\r\n    0.054263136289\r\n    28.1703556342\r\n    0.0815290055249\r\n    34.347978591\r\n    0.0346564716312\r\n    19.4157645742\r\n    0.0513183309038\r\n    17.6001812217\r\n    0.0635282078349\r\n    26.7531269043\r\n    0.207280952381\r\n    29.1256206323\r\n    0.209704465657\r\n    28.7894838321\r\n    0.275218501984\r\n    35.7230183845\r\n    0.18351927682\r\n    38.0406700795\r\n    0.244837752903\r\n    29.6979916062\r\n    0.736501244164\r\n    16.1596946558\r\n    0.167498023715\r\n    37.1591429276\r\n    0.173339194139\r\n    40.353371216\r\n    0.439834355882\r\n    18.6022942556\r\n    0.302364688057\r\n    19.4793492437\r\n    0.527624605989\r\n    19.4449607487\r\n    0.253293266326\r\n    16.3200322601\r\n    0.494475349602\r\n    16.1043635589\r\n    0.494324773881\r\n    17.4054929\r\n    0.539951862507\r\n    16.3336069378\r\n    0.488588172021\r\n    15.0300424789\r\n    0.57586548816\r\n    23.4609410282\r\n    0.658826944683\r\n    14.6767697053\r\n    0.688857930505\r\n    15.3383656766\r\n    0.622001952418\r\n    18.2203175257\r\n    0.256793181708\r\n    19.6571659013\r\n    \r\n\r\n    C:\\Anaconda2\\lib\\site-packages\\matplotlib\\figure.py:397: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\r\n      \"matplotlib is currently using a non-GUI backend, \"\r\n    \r\n\r\n\r\n![png](output_18_2.png)\r\n\r\n\r\n\r\n```python\r\nmodel_complexity(X_train, y_train, X_test, y_test)\r\n```\r\n\r\n    Creating a model complexity graph. . . \r\n    44.6808032477\r\n    54.1464733818\r\n    26.3461372216\r\n    25.6217609916\r\n    16.2582331003\r\n    17.7389846107\r\n    9.2767538123\r\n    12.4018317397\r\n    6.29491207432\r\n    13.9744288629\r\n    4.17594595812\r\n    24.0282388399\r\n    2.26401885362\r\n    14.7159087919\r\n    1.16963407345\r\n    14.7168338191\r\n    0.536368134455\r\n    13.0899681915\r\n    0.256793181708\r\n    12.7743317253\r\n    0.119861803059\r\n    18.7824213075\r\n    0.0359806967985\r\n    17.3337794408\r\n    0.00532519504977\r\n    18.3072154717\r\n    \r\n\r\n\r\n![png](output_19_1.png)\r\n\r\n\r\n# Model Prediction\r\nIn this final section of the project, you will make a prediction on the client's feature set using an optimized model from `fit_model`. When applying grid search along with cross-validation to optimize your model, it would typically be performed and validated on a training set and subsequently evaluated on a **dedicated test set**. In this project, the optimization below is performed on the *entire dataset* (as opposed to the training set you made above) due to the many outliers in the data. Using the entire dataset for training provides for a less volatile prediction at the expense of not testing your model's performance. \r\n\r\n*To answer the following questions, it is recommended that you run the code blocks several times and use the median or mean value of the results.*\r\n\r\n\r\n```python\r\n\r\n\r\nx = []\r\nfor i in range(1000):\r\n    x.append(reg.get_params()['max_depth'])\r\n    \r\nprint \"Final model has an optimal max_depth parameter of\", int(np.mean(x))\r\n```\r\n\r\n     Final model has an optimal max_depth parameter of 4\r\n    \r\n\r\n\r\n```python\r\nsale_price = reg.predict(CLIENT_FEATURES)\r\nprint \"Predicted value of client's home: {0:.3f}\".format(sale_price[0])\r\n```\r\n\r\n    Predicted value of client's home: 21.630\r\n    \r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}